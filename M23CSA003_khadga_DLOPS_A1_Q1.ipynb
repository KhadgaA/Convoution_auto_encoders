{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(in_features=4, out_features=5)\n",
    "        self.fc2 = nn.Linear(in_features=5, out_features=7)\n",
    "        self.fc_out = nn.Linear(in_features=7, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    log_writer=None,\n",
    "):\n",
    "\n",
    "    history = {\n",
    "        \"train\": {\"loss\": [], \"accuracy\": []},\n",
    "        \"valid\": {\"loss\": [], \"accuracy\": []},\n",
    "    }\n",
    "    n = len(train_loader)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        Loss_epoch = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        evaluation_train = {\"accuracy\": 0, \"loss\": 0}\n",
    "        for idx, data in enumerate(tqdm(train_loader)):\n",
    "            input, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            Loss_epoch += loss.item() * len(labels)\n",
    "            correct += accuracy(output, labels) * len(labels)\n",
    "            total += len(labels)\n",
    "        # Loss_history.append(Loss_epoch/n)\n",
    "        evaluation_train[\"accuracy\"] = correct / total\n",
    "        evaluation_train[\"loss\"] = Loss_epoch / total\n",
    "        # evaluation_train = evaluate(model, train_loader,criterion, device)\n",
    "        evaluation_valid, _ = evaluate(model, valid_loader, criterion, device)\n",
    "        print(f\"epoch: {epoch}, train: {evaluation_train}, valid: {evaluation_valid}\")\n",
    "\n",
    "        history[\"train\"][\"accuracy\"].append(evaluation_train[\"accuracy\"])\n",
    "        history[\"train\"][\"loss\"].append(evaluation_train[\"loss\"])\n",
    "        history[\"valid\"][\"loss\"].append(evaluation_valid[\"loss\"])\n",
    "        history[\"valid\"][\"accuracy\"].append(evaluation_valid[\"accuracy\"])\n",
    "\n",
    "        if log_writer is not None:\n",
    "            log_writer.log(\n",
    "                {\n",
    "                    \"train/train_loss\": evaluation_train[\"loss\"],\n",
    "                    \"train/train_accuracy\": evaluation_train[\"accuracy\"],\n",
    "                    \"val/valid_loss\": evaluation_valid[\"loss\"],\n",
    "                    \"val/valid_accuracy\": evaluation_valid[\"accuracy\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    _, preds = torch.max(output, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device, return_preds=False):\n",
    "    model.eval()\n",
    "    Accuracy_history = []\n",
    "    Loss_history = []\n",
    "    PREDS = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(data_loader):\n",
    "            input, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            Accuracy_history.append(accuracy(output, target))\n",
    "            Loss_history.append(loss.item())\n",
    "            if return_preds:\n",
    "                # PREDS.extend(torch.max(output, dim=1)[1].tolist())\n",
    "                PREDS.extend(output.tolist())\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": torch.mean(torch.Tensor(Accuracy_history)).item(),\n",
    "        \"loss\": torch.mean(torch.Tensor(Loss_history)).item(),\n",
    "    }, PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as tt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class IRISDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x  # torch.Tensor(x,).unsqueeze(1).type(torch.FloatTensor)\n",
    "        self.y = y  # torch.Tensor(y).type(torch.LongTensor)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X, iris_Y = datasets.load_iris(return_X_y=True)\n",
    "iris_X = iris_X.astype(\"float32\")\n",
    "iris_Y = iris_Y.astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 231.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3481481538878547, 'loss': 1.1214772286238495}, valid: {'accuracy': 0.30000001192092896, 'loss': 1.135094165802002}\n",
      "Kfold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 295.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.2814814895391464, 'loss': 1.0993227782072845}, valid: {'accuracy': 0.15000000596046448, 'loss': 1.083238124847412}\n",
      "Kfold: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 168.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.303703710436821, 'loss': 1.0975872631426211}, valid: {'accuracy': 0.44999998807907104, 'loss': 1.0786683559417725}\n",
      "Kfold: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 251.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3407407491295426, 'loss': 1.1260516511069403}, valid: {'accuracy': 0.20000000298023224, 'loss': 1.1488444805145264}\n",
      "Kfold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 251.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3407407491295426, 'loss': 1.0934633325647425}, valid: {'accuracy': 0.20000000298023224, 'loss': 1.1110225915908813}\n",
      "Kfold: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 299.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3703703780968984, 'loss': 1.0883596120057282}, valid: {'accuracy': 0.4000000059604645, 'loss': 1.0784785747528076}\n",
      "Kfold: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 252.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.6370370432182595, 'loss': 1.0859140025244818}, valid: {'accuracy': 0.800000011920929, 'loss': 1.0585875511169434}\n",
      "Kfold: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 286.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.2666666722959942, 'loss': 1.111108417864199}, valid: {'accuracy': 0.10000000149011612, 'loss': 1.1189820766448975}\n",
      "Kfold: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 249.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3333333377484922, 'loss': 1.1049202548133001}, valid: {'accuracy': 0.25, 'loss': 1.126150369644165}\n",
      "Kfold: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 222.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train: {'accuracy': 0.3555555608537462, 'loss': 1.0894309282302856}, valid: {'accuracy': 0.10000000149011612, 'loss': 1.1283395290374756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class custom_summary_writer:\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "\n",
    "    def log(self, metrics):\n",
    "        for key, value in metrics.items():\n",
    "            self.writer.add_scalar(key, value)\n",
    "            \n",
    "writer = custom_summary_writer(SummaryWriter(\"runs/iris_experiment_1\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler())\n",
    "\n",
    "kFold = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(kFold.split(iris_X)):\n",
    "    print(f\"Kfold: {fold}\")\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        iris_X[train_index],\n",
    "        iris_X[test_index],\n",
    "        iris_Y[train_index],\n",
    "        iris_Y[test_index],\n",
    "    )\n",
    "\n",
    "    X_train = pipeline.fit_transform(X_train)\n",
    "    X_test = pipeline.transform(X_test)\n",
    "\n",
    "    X_train_dataset = IRISDataset(X_train, y_train, tt.Compose([torch.tensor]))\n",
    "    X_test_dataset = IRISDataset(X_test, y_test, tt.Compose([torch.tensor]))\n",
    "\n",
    "    train_loader = DataLoader(X_train_dataset, batch_size=10, shuffle=True)\n",
    "    test_loader = DataLoader(X_test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    model = SimpleMLP(3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    history = train(\n",
    "        model, train_loader, test_loader, optimizer, criterion, device, epochs=1,log_writer = writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
