# -*- coding: utf-8 -*-
"""M23CSA003_khadga_DLOPS_A1_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f_169hbWoG_8zTE81jxwOSt92vBMzyzj
"""

import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm


def RMSE(output, labels):
    return torch.sqrt(torch.mean((output - labels) ** 2)).item()


def accuracy(output, labels):
    _, preds = torch.max(output, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()


def train(
    model,
    train_loader,
    valid_loader,
    optimizer,
    criterion,
    device,
    epochs=10,
    log_writer=None,
    verbose=True,
    eval_metrics=[accuracy],
    label_idx=-1,
):

    eval_metric_names = [
        (
            eval_metric.__name__
            if (_ := eval_metric.__class__.__name__) == "function"
            else _
        )
        for eval_metric in eval_metrics
    ]

    history = {
        "train": {"loss": []},
        "valid": {"loss": []},
    }
    for eval_metric_name in eval_metric_names:
        history["train"][eval_metric_name] = []
        history["valid"][eval_metric_name] = []

    n = len(train_loader)
    model.to(device)
    for epoch in range(epochs):
        model.train()
        Loss_epoch = 0
        correct = {eval_metric_name: 0 for eval_metric_name in eval_metric_names}
        total = 0
        evaluation_train = {
            "loss": 0,
            **{eval_metric_name: 0 for eval_metric_name in eval_metric_names},
        }

        for idx, data in enumerate(tqdm(train_loader, disable=not verbose)):
            input, labels = data[0].to(device), data[label_idx].to(device)
            # print(input.shape, labels.shape)

            optimizer.zero_grad()
            output = model(input)
            loss = criterion(output, labels)
            loss.backward()
            optimizer.step()

            Loss_epoch += loss.item() * len(labels)

            for eval_metric, eval_metric_name in zip(eval_metrics, eval_metric_names):
                correct[eval_metric_name] += torch.tensor(
                    eval_metric(output, labels)
                ).item() * len(labels)
            total += len(labels)

        for eval_metric_name in eval_metric_names:
            evaluation_train[eval_metric_name] = correct[eval_metric_name] / total

        evaluation_train["loss"] = Loss_epoch / total
        evaluation_valid, *_ = evaluate(
            model,
            valid_loader,
            criterion,
            device,
            eval_metrics=eval_metrics,
            label_idx=label_idx,
        )
        if verbose:
            print(
                f"epoch: {epoch}, train: {evaluation_train}, valid: {evaluation_valid}"
            )

        for eval_metric_name in eval_metric_names:
            history["train"][eval_metric_name].append(
                evaluation_train[eval_metric_name]
            )
            history["valid"][eval_metric_name].append(
                evaluation_valid[eval_metric_name]
            )

        history["train"]["loss"].append(evaluation_train["loss"])
        history["valid"]["loss"].append(evaluation_valid["loss"])

        if log_writer is not None:
            log_dict = {}
            for eval_metric_name in eval_metric_names:
                log_dict.update(
                    {
                        f"train/train_{eval_metric_name}": (
                            evaluation_train[eval_metric_name],
                            epoch,
                        ),
                        f"val/valid_{eval_metric_name}": (
                            evaluation_valid[eval_metric_name],
                            epoch,
                        ),
                    }
                )

            log_dict.update(
                {
                    "train/train_loss": (evaluation_train["loss"], epoch),
                    "val/valid_loss": (evaluation_valid["loss"], epoch),
                }
            )
            log_writer.log(log_dict)

    return history


def evaluate(
    model,
    data_loader,
    criterion,
    device,
    eval_metrics=[accuracy],
    return_preds=False,
    return_features=False,
    label_idx=-1,
):
    model.eval()
    eval_metric_names = [
        (
            eval_metric.__name__
            if (_ := eval_metric.__class__.__name__) == "function"
            else _
        )
        for eval_metric in eval_metrics
    ]
    metrics = {eval_metric_name: [] for eval_metric_name in eval_metric_names}
    Loss_history = []
    PREDS = []
    features = []
    ground_truth = []
    with torch.no_grad():
        for idx, data in enumerate(data_loader):
            input, target = data[0].to(device), data[label_idx].to(device)
            if return_features:
                output, feature = model(input, get_features=True)
                features.extend(feature.tolist())
            output = model(input)
            loss = criterion(output, target)

            for eval_metric, eval_metric_name in zip(eval_metrics, eval_metric_names):
                metrics[eval_metric_name].append(eval_metric(output, target))

            Loss_history.append(loss.item())
            if return_preds:

                PREDS.extend(output.tolist())
            ground_truth.extend(target.tolist())
    return (
        {
            **{
                eval_metric_name: torch.mean(
                    torch.Tensor(metrics[eval_metric_name])
                ).item()
                for eval_metric_name in eval_metric_names
            },
            "loss": torch.mean(torch.Tensor(Loss_history)).item(),
        },
        PREDS,
        features,
        ground_truth,
    )



from torchvision.datasets import DatasetFolder, ImageFolder
from skimage import color
from torchvision.transforms import v2 as tt
from torch.utils.data import DataLoader

data_transform = tt.Compose(
    [
        tt.ToImage(),
        tt.Resize(size=(224, 224)),
        tt.RandomHorizontalFlip(p=0.5),
        tt.RandomRotation(degrees=45),
        tt.ToDtype(torch.float32, scale=1 / 255),
    ]
)



from torchvision import datasets



train_data = datasets.ImageFolder(
    root="./hymenoptera_data/train", transform=data_transform, target_transform=None
)



test_data = datasets.ImageFolder(
    root="./hymenoptera_data/val", transform=data_transform
)



print(f"Train data:\n{train_data}\nTest data:\n{test_data}")

classes = train_data.classes

def collate_fn_lab(data):
    rgb_images, labels = zip(*data)
    lab_images = [
        torch.tensor(color.rgb2lab(torch.permute(x, (1, 2, 0)))).permute(2, 0, 1)
        for x in rgb_images
    ]
    return torch.stack(lab_images), torch.stack(rgb_images), torch.tensor(labels)

def collate_fn_negative(data):
    rgb_images, labels = zip(*data)
    neg_images = [
        torch.tensor(torch.abs(1 - torch.permute(x, (1, 2, 0)))).permute(2, 0, 1)
        for x in rgb_images
    ]
    return torch.stack(rgb_images), torch.stack(neg_images), torch.tensor(labels)

def collate_fn_flip(data):
    rgb_images, labels = zip(*data)
    flip_images = [
        torch.tensor(
            tt.RandomHorizontalFlip(p=1.0)(torch.permute(x, (1, 2, 0)))
        ).permute(2, 0, 1)
        for x in rgb_images
    ]
    return torch.stack(rgb_images), torch.stack(flip_images), torch.tensor(labels)

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import (
    precision_score,
    recall_score,
    accuracy_score,
    f1_score,
    confusion_matrix,
)


def get_features(model, data_loader, device, get_preds=False):

    model.eval()

    features = []

    labels = []

    preds = []

    with torch.no_grad():

        for idx, data in enumerate(data_loader):

            input, target = data[0].to(device), data[-1].to(device)

            output = model(input, get_features=True)

            if get_preds:

                preds.extend(output[0].tolist())

            features.extend(output[1].tolist())

            labels.extend(target.tolist())
    return features, labels, preds



def plot_TSNE(X, Y, title, writer=None):

    X_embedded = TSNE(
        n_components=2, perplexity=min(20, len(X)), n_iter=1000
    ).fit_transform(X)

    fig = plt.figure(figsize=(8, 8))

    ax = plt.subplot(111)

    scatter = ax.scatter(X_embedded[:, 0], X_embedded[:, 1], c=Y)

    legend1 = ax.legend(*scatter.legend_elements(), title="Classes")

    ax.add_artist(legend1)

    plt.title(f"TSNE model: {title}")

    plt.show()

    if writer is not None:

        writer.add_figure("TSNE", fig, global_step=0)


def plot_PCA(X, Y, title, writer=None):
    X_embedded = PCA(
        n_components=2,
    ).fit_transform(X)
    fig = plt.figure(figsize=(8, 8))
    ax = plt.subplot(111)
    scatter = ax.scatter(X_embedded[:, 0], X_embedded[:, 1], c=Y)
    legend1 = ax.legend(*scatter.legend_elements(), title="Classes")
    ax.add_artist(legend1)
    plt.title(f"PCA model: {title}")
    plt.show()
    if writer is not None:
        writer.add_figure("PCA", fig, global_step=0)


def add_pr_curve_tensorboard(
    class_index, classes, test_probs, test_label, writer, global_step=0
):
    tensorboard_truth = test_label == class_index
    tensorboard_probs = test_probs[:, class_index]

    writer.add_pr_curve(
        classes[class_index],
        tensorboard_truth,
        tensorboard_probs,
        global_step=global_step,
    )


def acc_pr_rec(preds, labels, classes, writer, global_step=0):
    preds = np.array(preds)
    labels = np.array(labels)
    preds_ = np.argmax(preds, axis=1)
    for i in range(len(classes)):
        add_pr_curve_tensorboard(i, classes, preds, labels, writer, global_step)
    writer.add_scalar(
        "Accuracy", ac := accuracy_score(labels, preds_), global_step=global_step
    )
    writer.add_scalar(
        "Precision",
        pr := precision_score(labels, preds_, average="weighted"),
        global_step=global_step,
    )
    writer.add_scalar(
        "Recall",
        rec := recall_score(labels, preds_, average="weighted"),
        global_step=global_step,
    )
    writer.add_scalar(
        "F1",
        f1 := f1_score(labels, preds_, average="weighted"),
        global_step=global_step,
    )
    writer.close()
    return ac, pr, rec, f1


def plot_ConfusionMatrix(preds, labels, classes, title, writer, global_step=0):
    cm = confusion_matrix(labels, np.argmax(preds, axis=1), normalize="true")
    fig = plt.figure(figsize=(6, 6))
    ax = plt.subplot(111)
    cax = ax.matshow(cm)
    fig.colorbar(cax)
    plt.xticks(range(len(classes)), classes, rotation=90)
    plt.yticks(range(len(classes)), classes)
    plt.title(f"Model: {title}")
    plt.show()
    writer.add_figure("ConfusionMatrix", fig, global_step=global_step)
    writer.close()
    return cm

from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio
from torch.utils.tensorboard import SummaryWriter


class custom_summary_writer:
    def __init__(self, writer):
        self.writer = writer

    def log(self, metrics):
        for key, value in metrics.items():
            self.writer.add_scalar(key, *(value))


class SimpleMLP(nn.Module):
    def __init__(self, n_classes):
        super().__init__()
        self.fc_in = nn.Linear(in_features=397832, out_features=512)
        self.fc_out = nn.Linear(in_features=512, out_features=n_classes)

    def forward(self, x, get_features=False):
        x = nn.Flatten()(x)
        x = self.fc_in(x)
        x = F.relu(x)
        x = self.fc_out(x)
        x = F.softmax(x, dim=1)
        if get_features:
            return x, None
        return x


class ConvAutoEncoder(nn.Module):
    def __init__(self, latent_dim=300):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 16, 3, stride=1, padding=1),
            nn.LeakyReLU(True),
            nn.BatchNorm2d(16),
            nn.Conv2d(16, 8, 3, stride=1, padding=1),
            nn.LeakyReLU(True),
            nn.AvgPool2d(2, stride=1),
            nn.BatchNorm2d(8),
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(8, 8, 2, stride=1),
            nn.BatchNorm2d(8),
            nn.ReLU(True),
            nn.ConvTranspose2d(8, 16, 3, stride=1, padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 3, 3, stride=1, padding=1),
            nn.BatchNorm2d(3),
            nn.ReLU(True),
        )

    def forward(self, x, get_features=False):
        y = self.encoder(x)
        x = self.decoder(y)
        if get_features:
            return x, y
        return x


for task, collat_fn in {
    "lab": collate_fn_lab,
    "negative": collate_fn_negative,
    "flip": collate_fn_flip,
}.items():
    print("-" * 20)
    print(f"Task: {task}")
    print("-" * 20)
    writer = custom_summary_writer(SummaryWriter(f"runs/CAE/task_{task}"))

    train_loader = DataLoader(
        dataset=train_data,
        batch_size=32,
        shuffle=True,
        collate_fn=collat_fn,
    )

    test_loader = DataLoader(
        dataset=test_data,
        batch_size=8,
        shuffle=False,
        collate_fn=collat_fn,
    )

    train_loader, test_loader

    model = ConvAutoEncoder()
    optimizer = optim.SGD(model.parameters(), lr=0.1)
    criterion = nn.MSELoss()
    device = "cuda"
    # writer = None
    verbose = True
    history = train(
        model,
        train_loader,
        test_loader,
        optimizer,
        criterion,
        device,
        epochs=10,
        log_writer=writer,
        verbose=True,
        eval_metrics=[
            RMSE,
            StructuralSimilarityIndexMeasure(data_range=1.0).to(device),
            PeakSignalNoiseRatio().to(device),
        ],
        label_idx=-2,
    )
    features_train_data, train_labels, _ = get_features(model, train_loader, device)
    features_test_data, test_labels, test_preds = get_features(
        model, test_loader, device, get_preds=True
    )
    features = np.concatenate([features_train_data, features_test_data])
    labels = np.concatenate([train_labels, test_labels])
    plot_TSNE(
        features.reshape(features.shape[0], -1),
        labels,
        title="CAE",
        writer=writer.writer,
    )
    plot_PCA(
        features.reshape(features.shape[0], -1),
        labels,
        title="CAE",
        writer=writer.writer,
    )

    for img, _, labels in test_loader:
        img = img.to(device)
        output = model(img)
        plt.imshow(_[1].permute(1, 2, 0).cpu().detach().numpy())
        plt.title("Original")
        plt.show()
        plt.imshow(output[1].permute(1, 2, 0).cpu().detach().numpy())
        plt.title("Reconstructed")
        plt.show()
        break

    print("*" * 20)
    print("Simple MLP Transfer Learning")
    print("*" * 20)
    n_splits = 1
    Avg_Accuracy = 0
    Avg_Loss = 0
    Avg_Precision = 0
    Avg_Recall = 0

    del model.decoder
    for param in model.encoder.parameters():
        param.requires_grad = False

    model.decoder = SimpleMLP(2).to(device)
    optimizer = optim.SGD(model.decoder.parameters(), lr=0.1)
    criterion = nn.CrossEntropyLoss()

    history = train(
        model,
        train_loader,
        test_loader,
        optimizer,
        criterion,
        device,
        epochs=10,
        log_writer=writer,
        verbose=True,
        eval_metrics=[accuracy],
        label_idx=-1,
    )
    evaluation_test, test_preds, _, test_labels = evaluate(
        model,
        test_loader,
        criterion,
        device,
        eval_metrics=[accuracy],
        return_preds=True,
        label_idx=-1,
    )

    loss_test = criterion(torch.tensor(test_preds), torch.tensor(test_labels)).item()
    ac, pr, rec, f1 = acc_pr_rec(test_preds, test_labels, classes, writer.writer)

    print(f"test accuracy: {ac}, test loss: {loss_test}")
    print(f"Precision: {pr}, Recall: {rec}, F1: {f1}")
    print("-" * 20)
    Avg_Accuracy += ac
    Avg_Loss += loss_test
    Avg_Precision += pr
    Avg_Recall += rec

    cm = plot_ConfusionMatrix(
        test_preds, test_labels, classes, "Simple MLP", writer.writer
    )

    writer.writer.close()
    print(
        f"Average Accuracy: {Avg_Accuracy/n_splits}, Average Loss: {Avg_Loss/n_splits}, Average Precision: {Avg_Precision/n_splits}, Average Recall: {Avg_Recall/n_splits}"
    )

